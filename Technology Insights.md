# A. FlashRL 

论文：https://arxiv.org/abs/2311.06917

博客：https://fengyao.notion.site/flash-rl

github：https://github.com/yaof20/Flash-RL

### 针对问题

训练的主要瓶颈常常不在“更新参数”，而在 rollout 生成。FlashRL 旨在**让 rollout 用 INT8/FP8 做量化推理提速/省显存，但训练仍保持全精度的稳定与效果**。

### 核心观点

洞察 A：RL 更新依赖采样动作的 logprob，训练侧与推理侧数值实现不一致，会把本应 on-policy 的训练退化为 off-policy，从而带来不稳定或性能损失。

洞察 B：量化 rollout 的收益取决于“模型规模 × 生成长度”，并不绝对有益。量化有额外开销，更推荐用在 14B+（尤其 32B）且长 CoT（如 DAPO）的场景。

### 技术实现

* TIS 修正 rollout–training mismatch：基于重要性采样思想对 policy loss 施加修正系数，并对系数做截断处理防止过修正。
* INT 8 在线量化：仅在训练开始时计算一次校准结果，以供后续复用。

# B. QuRL

论文：https://openreview.net/forum?id=eG0bpCwdKn

### 针对问题

序列由量化策略生成，但更新在全精度策略上发生，这种 mismatch 导致训练后期出现 collapse。RL 每 step 的参数更新很小（受 trust-region/clip 约束），小到被量化误差淹没，量化模型几乎无法感知这些更新。

### 技术实现

* ACR：Adaptive Clipping Range，动态调整 clip 的信赖域边界。仅使用 TIS 会导致 KL 变大和梯度估计偏差。原因在于目标函数后项会吸收外面的修正因子，导致裁剪区间变窄，会裁剪一些本不需要裁剪的 token。而 ACR
  保持一个固定的上界基准。
* UAQ：Update-Aware Quantization，问题在于 RL 的每步更新太小，小到会被量化噪声淹没。如果使用 PTQ 校准，开销大。如果使用 QAT，会引入训练图与推理引擎和 vLLM 之间更多实现差异，反而加剧 rollout/训练 mismatch。UAQ 对权重和激活使用等价缩放，实践经验选择缩放因子为 1.5。
* 工程上实践发现：**当前版本 vLLM 的 FP8 KV-cache 量化收益不明显**，因此实验里不启用 KV-cache 量化。

### 量化成果

模型越大，量化带来的吞吐收益越明显。7B 的 INT8 大多带来 20%–30% 加速；32B 在 A100 上约加速 70%、H100 上约加速 90%。

# C. QeRL

论文：https://www.arxiv.org/pdf/2510.11696

github：https://github.com/NVlabs/QeRL

### 针对问题

LLM 的 RL 常见瓶颈，rollout 慢、显存压不住、训练成本高。QeRL 用硬件友好的 **NVFP4（FP4）+ LoRA 把 rollout 与 prefilling 加速**，同时把“量化噪声”从副作用变成探索增强，在效率之外提升 RL 收敛与最终效果。

### 核心观点

洞察 A：量化噪声在 RL 里不完全有害，能提高探索。SFT 追求“拟合数据分布”，噪声常是坏事；RL 追求“发现高回报轨迹”，可控噪声能变成免费的探索机制。

洞察 B：噪声必须动态可控，否则静态量化误差难以匹配探索-收敛的阶段性需求。

### 技术实现

* QeRL 用 NVFP4 存权重，并在 rollout 与 prefilling 阶段集成基于 Marlin 的推理方式，保持精度的同时提升吞吐。
* AQN（Adaptive Quantization Noise）把静态量化误差变成可调探索。对每个量化线性层，每次 forward 都采样一个随机噪声；从较大的起始噪声逐步降到较小的结束噪声，以平衡探索与收敛；实验探究验证指数衰减更利于后期稳定高 reward。

### 对比 FlashRL

QeRL 的主张更激进，用 NVFP4 让 rollout 本身更快，并把量化带来的不确定性当作探索红利。

### 量化成果

rollout 阶段可达 >1.5× 的加速；并指出随着训练后期输出更长，速度优势会更明显。相对 QLoRA 约 1.8× 的端到端训练加速。

# D. Jet-RL

论文：https://arxiv.org/abs/2601.14243

### 核心观点

洞察 A：统一训练与 rollout 的 FP8 精度流，从源头消除数值 mismatch。

### 技术实现

* 训练与推理共享一致的量化行为，恢复 on-policy 一致性。
* 讨论了线性层三类 GEMM 的 FP8 量化布局与算子设计，并在训练中对 GEMM 进行 FP8 加速，同时在关键位置保持 BF16 以稳住收敛（例如梯度传输保持 BF16）。
* 用 vLLM 做 inference engine、VeRL 做 RL 训练框架，量化 GEMM 参考 DeepGEMM，并用 Triton 实现量化/transpose/融合算子。

### 量化成果

在 rollout 阶段实现了 33% 的加速，在 training 阶段实现了 41% 的加速，相较于 BF16 训练，端到端加速了 16% ，同时在所有设置中保持稳定的收敛，导致的精度下降可忽略不计。

# E. On the Rollout-Training Mismatch in Modern RL Systems

论文：https://openreview.net/pdf?id=8MHqvb4lK9

即使模型参数一致，不同 rollout 引擎（如 vLLM vs 训练 backend）在数值上产生的策略概率差异也会破坏 on‑policy RL 假设，尤其在量化 rollout 时更明显。

解决思路：使用 截断重要性采样 (Truncated Importance Sampling, TIS) 来修正 rollout distribution 与训练 distribution 之间的偏差。

# F. RILQ （“Rank‑Insensitive”（秩不敏感），关于量化误差）

论文：https://arxiv.org/pdf/2412.01129

### 背景

LoRA（Low‑Rank Adaptation）作为参数高效微调（PEFT）方法，也被用来补偿量化带来的模型精度下降，称为 LoRA‑based Quantization Error Compensation（LQEC）。然而，在低位量化下传统 LQEC 方法表现欠佳。

RILQ 论文提出了一个新的视角：不是对每一层单独补偿，而是从模型整体激活输出误差(Rank‑Insensitive Loss)出发，通过协同调整跨层的 LoRA 适配器来补偿量化误差，从而实现在低秩设置下仍能恢复精度。

### 核心问题

量化误差可以理解为量化后输出与高精度输出的差异：传统低秩补偿假设 E 是低秩矩阵，可以通过少量的低秩适配器来逼近。但论文指出在超低位量化下，这种误差往往是高秩的；在**单层或单线性模块**输出级别进行误差对齐时，对秩的敏感性较高，需要更高的秩才能有效补偿；而当对**整体模型输出层**（模型级输出）进行对齐时，误差的秩敏感性明显降低。

跨层协同补偿能让局部不能补偿的高秩误差通过全模型多层的调整来缓和，从而不那么依赖高秩表示。

### 核心方法

* 模型级激活差异损失：和之前的 LoRA 误差补偿不同之处在于它的损失函数：传统的补偿优化基于局部层级损失，例如单个线性模块，而 RILQ 提出更大的优化范围即在整个模型的最终输出上对齐。
* 双目标优化:为了同时保证生成能力与误差补偿效果，RILQ 采用联合优化目标 $L=L_{model}​+β⋅L_{GT}$
* 跨层协同调整 LoRA 适配器:这种协作能在整体上补偿高秩量化误差，而不需要大量高秩参数。

### 与 RL 低精度 Rollout 的关联与启发

RILQ 根据全局模型级损失来对抗高秩误差，与 RL 中通过 rollout 全段误差来校正策略误差有相似逻辑。

启发：从全局策略输出对齐的角度设计误差损失可能比局部 rollout 误差更稳定。

**低精度 rollout 中，误差控制手段分析：**

1. 局部误差控制

在低精度训练和推理中，局部误差控制通常是指在每个时间步或单独模块（如神经网络中的每一层或每一模块）内进行误差补偿。这种方式的典型策略包括：

* 量化误差局部补偿：在每个层级中，通过调整量化误差来恢复误差带来的影响。
* 量化感知训练：通过模拟低精度误差，并在每一层引入误差校正项，让每一层在训练时尽量适应量化误差。通常是局部进行，每一层独立对抗量化噪声。
* 梯度修正：在低精度的推理阶段，每个时间步都会对梯度进行修正，以减少由量化误差引起的影响。

2. 全局误差控制
 
* 全局损失函数设计：通过在强化学习中引入“累计误差”，使得整个策略路径中的误差都能在训练中被反馈并校正。通常这种方法适用于多步推理或长时序决策任务。
* 全局回归修正：类似于回归损失，全局回归修正通过对整个任务的预测误差进行最优化，使得低精度推理中的误差不会被局部误差放大，从而在整个策略路径中进行全局修正。
* 自适应误差反馈机制：例如在强化学习中，可以通过自适应量化噪声（AQN）控制整体误差，而不是逐层修正。这种方法通过动态调整误差的补偿系数，使得整个模型在多次rollout中都能够适应低精度带来的误差。​

# G. What Makes Low-Bit Quantization-Aware Training Work for Reasoning LLMs?

### 背景

**后训练量化**会导致推理精度显著下降，尤其在推理任务中，性能下降尤其严重。论文关注通过 QAT 恢复低精度量化带来的性能下降，并特别关注推理任务中低精度 QAT 的效果。

### 核心问题

* 哪种训练目标最适合推理模型的 QAT？
* 在低精度设置下，如何提高 QAT 的训练效率？
* QAT 与强化学习（RL）结合时，低精度量化的效果如何？
* QAT 训练数据的选择如何影响推理模型的量化性能？

### 关键结论

* 在 QAT 训练中，知识蒸馏比传统的监督微调更有效。
* 使用 GPTQ 进行初始化能够显著提高 QAT 的训练效率，并加速收敛。实验结果表明，使用 GPTQ 初始化的模型比 RTN（对称量化基准）初始化的模型具有更高的初始精度和更快的收敛速度，从而减少了 QAT 训练的计算开销。
* 需要通过合适的冷启动机制（如使用 KD 初始化）来避免训练崩溃。
* 当训练数据与量化校准数据的领域一致时，QAT 模型能够更快收敛，并且最终精度更高。

# H. 低精度 RL 训推经验贴总结

1. vLLM 对低精度格式的支持与使用方式

支持的低精度类型：vLLM 已支持多种 8 比特低精度推理格式，包括 FP8（8 位浮点）和 INT8 量化模式，覆盖纯权重量化及权重 + 激活同时量化。

使用方式：vLLM 提供了**离线量化**和**在线量化**两种工作流。

对于离线量化，推荐使用 llm-compressor 工具对模型进行一次性量化，然后加载量化后的模型权重。

对于在线动态量化，vLLM 支持在推理引擎中对加载的模型实时量化。用户可通过环境变量或 API 参数开启。例如，在 FlashRL 中使用环境变量控制 vLLM 在线量化：export FLASHRL_CONFIG=fp8_vllm 开启 FP8 在线量化，或 FLASHRL_CONFIG=int8 开启 INT8 量化。FP8 动态量化无需校准且硬件友好，是首选方案；INT8 则需结合校准或 QAT 技术，在推理引擎和训练框架间同步更新量化权重。

混合精度策略如 W8A16，这种权重量化方法降低了一半模型内存，但计算时需将权重反量化为 FP16 参与乘法，因而在计算密集场景收益有限。

2. 利用 FP16 克服训练-推理不匹配问题  https://www.emergentmind.com/papers/2510.26788#related-papers

训练策略和推理策略之间的数值不匹配根本原因在于浮点精度本身。广泛采用的 BF16 浮点数虽然动态范围大，但会引入较大的舍入误差，从而破坏训练和推理之间的一致性。**改回 FP16 浮点数即可有效消除这种不匹配**。

实验结论：

* 在不同设置下 BF16 和 FP16 之间的训练奖励比较，表明 FP16 具有更优异的稳定性和收敛性。
* FP16 在标记和序列级别上显著降低了训练-推理不匹配。
* 从 BF16 切换到 FP16 可以稳定和延长 RL 训练，FP16 的性能优于所有 BF16 基线。

3. rollout 校准小结

| 维度 | Decoupled mode | Bypass mode | Bypass + Policy Gradient mode |
|------|------------------|--------------|-------------------------------|
| 策略数量 | 3 个：π_rollout, π_old, π_θ | 2 个：π_rollout = π_old, π_θ | 2 个：π_rollout, π_θ |
| rollout 策略 | π_rollout | π_rollout (= π_old) | π_rollout |
| 是否存在 π_old anchor | ✅ 有（独立存在） | ✅ 有（等于 rollout） | ❌ 没有 |
| 是否使用 PPO clipping | ✅ 使用 | ✅ 使用 | ❌ 不使用 |
| 本质算法 | 标准 PPO（off-policy 扩展） | 标准 PPO（近似 on-policy） | Off-policy Policy Gradient |
| 理论稳定性 | ⭐⭐⭐⭐ 很高 | ⭐⭐⭐ 高 | ⭐⭐ 较低 |
| 更新是否受约束 | ✅ 强约束（clipping） | ✅ 强约束（clipping） | ❌ 基本无约束 |
| 是否容易大步更新 | ❌ 不容易 | ❌ 不容易 | ✅ 很容易 |
| 方差水平 | 中 | 低 | 高 |
| 工程复杂度 | 高（维护三策略+IS） | 低 | 中 |















